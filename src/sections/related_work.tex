\section{Related Work}

\subsection{Autonomous Drone Landing}

Wynn~\cite{wynn} has developed a method for landing on a moving platform using fiducial markers to track the landing platform, with the initial aid of GPS. A larger marker allows recognition of the landing platform from long distances. A smaller marker of the same form is embedded inside the larger marker to allow for identification at close distances. After the landing platform is localized, different control states direct the drone's approach towards the marker - first causing the drone to approach quickly in the x and y dimensions, while maintaining a sufficient altitude above the marker (in the z dimension), and then gradually lowering to a small distance above the marker. At this point, the drone commits to a landing and lowers itself until detecting a successful landing, since the proximity of the camera to the landing pad means that the marker is no longer fully contained within the field of view of the camera, and thus can no longer be tracked. Other control states include switching from \textit{patrol mode} to \textit{tracking mode} once the relevant marker has been detected continuously for a small amount of time, and aborting a landing if the marker has not been detected for 2 seconds continuously. This method also takes into account the swaying of the landing platform itself, which is mounted on a barge.

Borowczyk et~al.~\cite{high_velocity_landing} have implemented a system allowing a DJI Matrice to land on a golf cart using  {GPS} with wirelessly transmitted position. The drone uses a {PN} system for initial approach. This initial approach is carried out at a fixed altitude and a gimbal-mounted camera is used for initial detection of a fiducial marker mounted to the landing platform. A fixed, downward-facing camera then detects the visual fiducial marker and a  {PID} controller manages close-range approach. A constant descent velocity is set during the final phase of landing. This method allows for successful landing on a platform moving at speeds of up to 50 km/h. Recommended future work includes using multiple fiducial markers of different sizes to identify the landing platform, as well as a single gimbal-mounted camera instead of the dual camera setup.

Falanga et~al.~\cite{vision_based_x_platform} outline a method for landing a quadrotor running the PX4 autopilot software on a moving platform indoors. The landing platform is fitted with a specific marker made up of a cross and a circle. The drone uses 2 cameras, the first mounted straight down from the drone, and the second mounted at a 45 degree angle down and towards the front of the drone. The images from these cameras are used to solve a  {PnP} problem which finds the relative pose of the landing platform's marker. A distance sensor helps to scale the vision-based pose estimation. The onboard computer determines optimal approach trajectories using this information. A Kalman filter makes the process robust to missed detections and helps to determine the velocity of the landing platform. Successful landings were reported with the landing platform moving up to 1.2 m/s.

Wubben et~al.~\cite{accurate_landing_UAV_ground_pattern} use the typical setup of a hexacopter drone with a single camera in a fixed, downward facing orientation to identify a landing platform via 2 ArUco markers. A Pixhawk controls the drone using ArduPilot, and a Raspberry Pi handles image processing and fiducial identification. The method reports successful and accurate landings, but also occasional failures due to visual loss of the landing platform. This visual loss was caused by sudden gusts of wind which pushed the drone away from the landing platform and out of the fixed camera's view.

Pluckter et~al.~\cite{drone_landing_unstructured_environments} have developed a method for precisely landing a drone in an \textit{unstructured} environment, which is to say an environment that has not been significantly artificially marked. On takeoff, the drone visually captures key points of interest in its environment. It then performs its mission and returns to a location above its takeoff location using  {GPS}. Subsequent visual analysis of the surroundings and comparison of this information with the similar information captured at takeoff allow the drone to localize itself. This process is continued throughout the entire landing. This method is interesting and significantly different from the fiducial methods, but only allows the drone to land at the takeoff location.

Polvara et~al.~\cite{drq_landing} introduce a method of training and testing {DQN} for landing drones in a simulated environment with simple outputs (left, right, forward, back, land), feeding the networks low-resolution images as input. Multiple networks were trained for specialized tasks, such as policy control, approach, and descent. The method performed with only slightly less accuracy than the conventional vision-based methods which use fiducial markers. However, the caveat is that the conventional methods can fail when the fiducial marker cannot be detected, whereas the method presented by Polvara et al still achieved a relatively high success rate. The method is, however, very simple and requires significant modification before it is viably applicable in a physical drone setting.

Cocchioni et.~al~\cite{autonomous_landing_and_recharge} present a custom landing pad and marker to allow a quadcopter to land and recharge autonomously. The marker is a black circle with a smaller black triangle embedded in its white center, which allows pose estimation in 6 degrees of freedom. A downward-facing camera detects the marker and PID systems control the position of the drone to direct it onto the landing pad. A downward-facing ultrasonic distance sensor provides the altitude of the drone above the landing pad. The landing pad has 4 conical indentations in it - one for each of the drone's legs. The idea is that the drone can slide into the ideal position passively if each of the legs can fall into its hole on the initial landing. Then, charging terminals attached to each leg make contact with terminals in the landing pad, and charging can take place. The method is successful in 95\% of cases, but involves a very specialized landing pad and custom fiducial marker, making distribution difficult.

\subsection{Landing Site Evaluation}

Desaraju et~al.~\cite{rooftop_landing} discuss a method for exploration and evaluation of rooftop landing sites using motion stereo image processing. In this method, a downward-facing monocular camera regularly captures images of the terrain below. Disparity maps and feature matches between pairs of images provide extrinsic camera parameters which allow the system to mimic stereo processing and recreate a 3D model of the terrain below. The system then assigns confidence levels to each unit of area in the 3D model which correspond to that area's viability as a landing site, based on the radius of clear terrain, absence of obstacles, and certainty of the information describing the landing site. Trajectories are generated to direct the drone to each landing site and are ranked by several metrics including required distance and angular velocity (to minimize the variation in the camera's orientation). The method is successful in distinguishing the rooftop from the ground and identifying viable rooftop landing sites but is computationally expensive and therefore requires some adjustment to run in real time.

Patterson et~al.~\cite{timely_landing_site_identification} introduce a method of detecting safe landing zones (SLZs) in single RGB images (not time sequences of images). The algorithm first attempts to find large areas in images taken from a downward-facing camera using Canny edge detection, and selecting large regions with no edges. The assumption is that edges correspond to physical region boundaries which represent obstacles. The edges are then ``dilated'' in order to create a safety buffer around the potential landing zones. These potential landing zones receive a ``safety score'' based on minimization of the possibility of human casualty, minimization of the possibility of property damage, and maximization of the possibility of survival for the drone and payload. Terrain is classified manually on a data set of 490 images with 9 terrain classes. Distance to man-made structures is a second component of the safety score, which is determined from analysis of existing maps. Lastly, a measure of roughness determined by the standard deviation of pixels from the mean of a safe landing zone, is used to determine which regions are smooth enough for landing. Fuzzy logic categorizes potential landing zones into ``unsuitable,'' ``risky,'' and ``suitable.'' 100 aerial images were manually labeled and SLZs were identified within them. The algorithm correctly identified the regions 94\% of the time with the knowledge of the existing maps, but reported about 10\% false positives without this knowledge.

Whalley et~al.~\cite{field_testing_helicopter} use LADAR to model terrain below a large model helicopter (182 pounds) in order to identify possible landing sites and trajectories. The required components weighed 6 pounds and consumed 2.9 A and 12 V. The system successfully avoided obstacles and selected landing sites, but no in-depth information on the landing site evaluation algorithm were given.

Johnson et~al.~\cite{helicopter_hazardous_terrain} use structure from motion (SfM) to generate terrain maps of the area below a drone. Features selected from input images allow the ground station computer to calculate relative motion between sequential images. Image segmentation enforces that the features should be dispersed relatively evenly in the dimensions of the image. The dense structure of the terrain below creates an estimate of the terrain map, from which the computer can extract steep slopes and other hazards. This terrain is then labeled by the perceived hazard rating at each pixel, and safe landing sites are determined by minimal roughness and sufficient largeness. The algorithm runs in less than 1 second on a 400 MHz processor. A total of 4 safe landings were achieved.

Garg et~al.~\cite{landing_cues} develop both a monocular method and stereo method to classify landing sites based on their viability, and to land at safe landing sites. The monocular method uses planar homography to generate a dense representation of terrain below a drone. It allows the drone to distinguish between rigid surfaces like ground and non-rigid surfaces like water. The stereo approach allows depth to be extracted from the images, which are also analyzed to identify roughness and slopes. The approaches disable landing if the environment detected is deemed unsuitable. The methods were not robust to surfaces that were covered in leaves because their changing nature led the methods to determine that they were not rigid.

Matunara and Scherer~\cite{conv_3d_lidar_landing} present a method to identify safe landing sites for autonomous helicopters using a 3D convolutional neural network. The data set is partially synthetic, and partially hand-labeled. Their method is able to identify low-height vegetation as a safe landing site, whereas conventional methods of analysis would label them as too rough. In this case they are viewed as ``porous'' instead of ``unsafe.'' The method is able to analyze successfully a cubic meter of ground in less than 5ms. It is tested on real and synthetic data, but not embedded onto a physical drone for real world landing tests.

\subsection{Summary}

Many methods exist for autonomous drone landing,
with some depending on special infrastructure to mark a known landing pad,
and with others analyzing terrain to identify previously unknown landing sites.
Infrastructure at known landing pads can be active (e.g.~radio/IR beacons, cameras, etc.) or passive (e.g.~visual fiducial markers).
Methods for identifying previously unknown landing sites depend on sensor-based terrain analysis or visual matching
and typically use RGB, RGBD, or LIDAR/LADAR to collect information about the ground below.
Methods of analyzing the sensor data range from very finely human-tuned methods of edge detection/dilation in images,
to black-box deep learning methods.
Additionally, older methods tend to use only real-world data, while newer methods begin to leverage quickly-generated, cheap, synthetic data from simulators.
Some methods can use single images, while others require images from multiple locations in order to deduce the underlying 3D structure of the terrain.
In almost all cases, the sensors are fixed rigidly to the drone, instead of being mounted on a gimbal.
It is common to approach the general location of a landing site using GPS,
and then to carry out the landing using some augmented sensing.
Processor-heavy methods cannot necessarily run in real time on a drone, and must either be run offline as a proof of concept,
or must be offloaded to a sophisticated ground control station for real time processing on bulky hardware.