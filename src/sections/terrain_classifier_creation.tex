\section{Terrain Classifier Creation}

We will create multiple methods and evaluate them on their ability to predict the
safety mask of the landing sites in the synthetic data sets.
We will focus first on using U-nets to map from the input LIDAR or RGBD image
to the output mask, as they have been proven to be good classifiers in many different scenarios,
from terrain analysis, to tissue analysis, to road identification, etc.
We anticipate that the main challenge in this domain will not be
to train the networks, but rather to keep them small enough
so that they can execute quickly on embedded hardware.
Nevertheless, we will start by focusing strictly on maximizing the ability of the networks
to accurately predict the safety masks.

\subsection{Pre-Processing}

An unavoidable step in this process is preprocessing the input data.
In the case of RGBD images, we should rectify input to remove sensor distortion.
This is helpful for understanding what the data truly represents,
and also removes sensor biases, so that the terrain analysis methods do not overfit to data from a particular sensor.
We will also transform input according to IMU tags in order to properly identify slants.
One option for pre-processing in the case of LIDAR point clouds is to convert them to depth images,
and we anticipate that it will be generally less computationally expensive to process depth images instead of point clouds.
In either case, the input must be normalized with respect to sensor calibration data -
sensors tend to auto-adjust their scales in order to clarify differences in values at each point.
This occurs when RGB cameras auto-adjust brightness in order to make a clearer picture,
and also when an RGBD camera detects the average range of detected depths, and adjusts its format accordingly.
Finally, downsizing input images and point clouds is a common method of computational speedup
that will most likely be necessary, given that the target hardware platforms are limited, embedded boards.

\subsection{Topographical Analysis}

The most important aspect of a safe landing site is its topography.
We want to identify regions of the pre-processed input images that are sufficiently flat and large.
Signal processing methods, such as analysis of the image's gradient, and even edge detection algorithms (e.g.~Canny, etc.)
can be used to detect contiguous flat regions.
Deep learning methods such as pooling and 2D convolutions can accomplish similar tasks.
The most relevant data channel in an RGBD image for topographical analysis is the depth channel,
but other channels could still offer useful information.
While it is possible to ``hand-make'' a solution for finding a landing region that is topographical viable,
it may also be useful to train deep learning methods to do this task.
They will likely have inherent biases that are different from the hand-made solutions,
which will give a wider view of the solution space in the worst case,
and which will yield better results in the best case.
Moreover, a deep learning method will be more suited to use the embedded TPU and GPU on the target companion boards,
and therefore may run faster.

\subsection{Semantic Segmentation}

One key application of image processing with deep learning is to segment images into the things they represent.
This would offer a huge benefit in terms of autonomous landing, as the system could recognize e.g.~water as unsafe,
and leaves as safe, regardless of their topography.
In general, it is expensive to label real world data sets for semantic segmentation because it must be done by hand.
However, the segmentation masks in AirSim allow quick generation of large, semantically labeled data sets.
Therefore, we will supplement the topographical analysis with semantic segmentation in the hopes of getting better results.

We will use a conventional CNN, U-net, residual U-net, for basic semantic segmentation of the pre-processed input images.
CNNs can semantically segment scenes based on kernels, U-nets use both convolutions and an encoder-decoder architecture,
and residual U-nets
All of these architectures are effective for semantic image segmentation, but in the context of embedded drone landing,
we have to strike a balance between the more complex networks that offer more intricate segmentation,
and the less complex networks that are likely to run faster.
The viability of each of the approaches must be empirically tested on the target platforms.

As an example of how this will work, we take an example from my project in RU's Deep Learning
course, wherein my group member and I used a Residual U-net to classify defects
in manufactured steel.
The data is a set of 17,000 images that are labeled by hand, according to which of 4 defects they contained.
Figure~\ref{figure:example_steel_prediction} shows an example of the input data from this project,
where the yellow and blue outlines show the locations of different types of defect areas.
An example output mask can be seen in Figure~\ref{figure:example_steel_prediction}.
This project also gave some insight into accuracy metrics that can help bias the output
for/against false positives/negatives.
For example, in the case of the steel defect recognition, we used a Tversky metric to
reduce the amount of false negative defect classifications because a second step of the
process is for a human to manually examine the defect-classified steel before discarding it.
In this context it is better to make the network more ``cautious,'' in the hopes that
alerting the human to some false defects will mean that the network never incorrectly approves
defected steel for sale.
Conversely, in the case of autonomous drones,
we would like the networks to be conservative in their identification of safe landing sites,
so that they never direct the drone to land in unsafe landing sites, even though they may reject
some landing sites that are actually safe.
This is purely to give an intuition of the form of the problem we will use the classifiers to solve.

\begin{figure}
    \centering
    \includegraphics[width=\textwidth]{images/original_image.png}
    \subfloat[Defect 1 Mask]{\includegraphics[width = 0.5\textwidth]{images/true_def0.png}}
    \subfloat[Defect 1 Prediction]{\includegraphics[width = 0.5\textwidth]{images/pred_def0.png}}\\
    \subfloat[Defect 2 Mask]{\includegraphics[width = 0.5\textwidth]{images/true_def1.png}}
    \subfloat[Defect 2 Prediction]{\includegraphics[width = 0.5\textwidth]{images/pred_def1.png}}\\
    \subfloat[Defect 3 Mask]{\includegraphics[width = 0.5\textwidth]{images/true_def2.png}}
    \subfloat[Defect 3 Prediction]{\includegraphics[width = 0.5\textwidth]{images/pred_def2.png}}\\
    \subfloat[Defect 4 Mask]{\includegraphics[width = 0.5\textwidth]{images/true_def3.png}}
    \subfloat[Defect 4 Prediction]{\includegraphics[width = 0.5\textwidth]{images/pred_def3.png}}
    \caption{A visualization of the network's performance in predicting defects.}
    \label{figure:example_steel_prediction}
\end{figure}

\subsection{Target Output}
\label{section:target_output}

The results of the topographical analysis and semantic segmentation will pass to
a hand-made wrapper
or subsequent network layer
that can combine them in order to create a mask of the predicted safety metrics of the terrain below the drone.
This safety mask will likely depend mostly on the topographical analysis, with the semantic segmentation giving the ability
to ``veto'' potentially smooth but dangerous regions like water, and classify as safe somewhat rough but potentially safe regions like leaves.